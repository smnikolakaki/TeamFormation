{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Implementation of clustering aggregation algorithms i) Neighbors ii) Hierarchical iii) KwikSort\n",
    "# Input: People with features, size k\n",
    "# Output: Clusters of size close to k\n",
    "# Method: Clustering Aggregation\n",
    "# Problem Formulation: Given clusterings C1,C2,...,Cm split the aggreement graph into parts of size close to k\n",
    "# (within some approximation) such that you maximize the sum of the weights of the edges within the clusters.\n",
    "# The weight of edge e={u,v} is the fraction of clusterings that put u and v in the same cluster\n",
    "# - maximizes the agreement.\n",
    "# Purpose: Creating teams for hackthegap\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from itertools import chain\n",
    "from itertools import izip\n",
    "from constrained_kmeans import KMeans\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import jaccard_similarity_score as JS\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: user ids\n",
    "# Output: Undirected graph where each user is a node and each user has an edge with each other user\n",
    "def initGraph(features):\n",
    "    users = list(features.keys())\n",
    "    # makes graph complete\n",
    "    edges = combinations(users, 2)\n",
    "    \n",
    "    # creating the graph\n",
    "    user_graph=nx.Graph()\n",
    "    user_graph.add_nodes_from(users)\n",
    "    user_graph.add_edges_from(edges)\n",
    "    \n",
    "    return user_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_computation(weight_metric,features_user1,features_user2):\n",
    "    \n",
    "    weight = float(\"inf\")\n",
    "    if weight_metric == 'difference':\n",
    "        weight = 1 - (abs(features_user1 - features_user2)/float(2))\n",
    "    elif weight_metric == 'jaccard':\n",
    "        intersect = len(list(set(features_user1) & set(features_user2)))\n",
    "        union = len(list(set(features_user1) | set(features_user2)))\n",
    "        weight = intersect / float(union)\n",
    "    elif weight_metric == 'same':\n",
    "        if isinstance(features_user1,int):\n",
    "            if features_user1 == features_user2:\n",
    "                weight = 1\n",
    "            else:\n",
    "                weight = 0\n",
    "        else:\n",
    "            idx1 = features_user1.index(1)\n",
    "            idx2 = features_user2.index(1)\n",
    "            if idx1 == idx2:\n",
    "                weight = 1\n",
    "            else:\n",
    "                weight = 0\n",
    "            \n",
    "    return weight      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: feature vectors\n",
    "# Output: weighted clusterings based on features, mapping between user pair and weights in different clusterings\n",
    "# Note!! 0: NumberOfHackathons, 1-6: Skills, 7: Beginner, 8-14: Interests. These are the distinct different features\n",
    "# that lead to the corresponding number of clusterings i.e. 4\n",
    "\n",
    "def create_clusterings(features,user_graph):\n",
    "    \n",
    "    clusterings = []\n",
    "    clustering_weights = {}\n",
    "    \n",
    "    # indexes that show where each feature ends, for instance\n",
    "    # feature skill can contain several values whose indexes range from 1-6\n",
    "    indexes = [0,6,7,14]\n",
    "    weight_metrics = ['difference','jaccard','same','same']\n",
    "    \n",
    "    # creating the weights for each clustering  - all weights range from 0 to 1\n",
    "    # numhackathons clustering (difference): weight = 1 - (|num1 - num2|/2)\n",
    "    # skills clustering (jaccard): Jaccard(set1,set2)\n",
    "    # beginner clustering (same): 1 if both beginners, or if both non-beginners, otherwise 0\n",
    "    # interest (same): 1 if same interest, otherwise 0\n",
    "    \n",
    "    counter = 0; values = []\n",
    "    for idx, value in enumerate(indexes):\n",
    "        user_graph_copy = nx.Graph(user_graph)\n",
    "        \n",
    "        weight_metric = weight_metrics[idx]\n",
    "        \n",
    "        for user1,user2 in user_graph_copy.edges(data=False):\n",
    "            key = (user1,user2)\n",
    "            # print 'Key is:',key\n",
    "            if idx == 0 or (indexes[idx] - indexes[idx-1]) == 1: \n",
    "                features_user1 = features[user1][idx]\n",
    "                features_user2 = features[user2][idx]\n",
    "                weight = weight_computation(weight_metric,features_user1,features_user2)\n",
    "                user_graph_copy.edge[user1][user2]['weight'] = weight\n",
    "            else:\n",
    "                features_user1 = features[user1][indexes[idx-1]+1:indexes[idx]+1]\n",
    "                features_user2 = features[user2][indexes[idx-1]+1:indexes[idx]+1]\n",
    "                weight = weight_computation(weight_metric,features_user1,features_user2)\n",
    "                user_graph_copy.edge[user1][user2]['weight'] = weight\n",
    "                \n",
    "            if key in clustering_weights:\n",
    "                clustering_weights[key].append(weight)\n",
    "            else:\n",
    "                clustering_weights[key] = [];\n",
    "                clustering_weights[key].append(weight)\n",
    "                              \n",
    "        clusterings.append(user_graph_copy)\n",
    "        \n",
    "    return clusterings, clustering_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggrement_Graph: Creating the aggreement graph used by the proposed team formation algorithms \n",
    "# Input: data\n",
    "# Output: Aggreement Graph \n",
    "# Note!! 0: NumberOfHackathons, 1-6: Skills, 7: Beginner, 8-14: Interests. These are the distinct different features\n",
    "def aggreement_graph(features):\n",
    "    print 'Creating aggreement graph'\n",
    "    # creating initial unweighted graph\n",
    "    user_graph = initGraph(features)\n",
    "    aggreement_graph = nx.Graph(user_graph)\n",
    "    \n",
    "    (clusterings, clustering_weights) = create_clusterings(features,user_graph)\n",
    "    for key,value in clustering_weights.iteritems():\n",
    "        user1 = key[0]; user2 = key[1]\n",
    "        value[1] = value[1]*2\n",
    "        value[2] = value[2]*5\n",
    "        value[3] = value[3]*10\n",
    "        \n",
    "        weight = sum(value)/float(len(value)+2+5+10)\n",
    "        aggreement_graph.edge[user1][user2]['weight'] = weight\n",
    "    \n",
    "    for u in aggreement_graph.nodes():\n",
    "        aggreement_graph.node[u]['size'] = 1\n",
    "    \n",
    "    return aggreement_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_nodes(u,v,Merge_AGraph,algo):\n",
    "    size = float(\"inf\")\n",
    "    new_node = ()\n",
    "    # 1) create a new cluster node with a) updated cluster nodes (u+v), b) updated cluster size\n",
    "    if algo == 'kwiksort':\n",
    "        if isinstance(v,tuple):\n",
    "            size = len(v) + 1\n",
    "            new_node = (v,u)\n",
    "            new_node = tuple(list(new_node[0]) + [new_node[1]])\n",
    "        else:\n",
    "            size = 2\n",
    "            new_node = (v,u)\n",
    "            # print '2 new node:',new_node\n",
    "            \n",
    "    elif algo == 'hierarchical':\n",
    "        new_node = (v,u)\n",
    "        new_node = tuple(chain(*(i if isinstance(i, tuple) else (i,) for i in new_node)))\n",
    "        size = len(new_node)\n",
    "        \n",
    "    # adding new node to graph    \n",
    "    Merge_AGraph.add_node(new_node)\n",
    "    Merge_AGraph.node[new_node]['size'] = size\n",
    "    # 2) for each neighbor of u and v compute average weight edge - Should not matter \n",
    "    # if it is u or v because it is a complete graph\n",
    "\n",
    "    neighbors_u = Merge_AGraph.neighbors(u)\n",
    "    \n",
    "    for neighbor in neighbors_u:\n",
    "        flag = False\n",
    "        if Merge_AGraph.has_edge(u,neighbor):\n",
    "            weight_u = Merge_AGraph.edge[u][neighbor]['weight']\n",
    "        else:\n",
    "            flag = True\n",
    "        if Merge_AGraph.has_edge(neighbor,v):    \n",
    "            weight_v = Merge_AGraph.edge[neighbor][v]['weight']\n",
    "        else:\n",
    "            flag = True\n",
    "\n",
    "        if flag == False:\n",
    "            new_weight = (weight_u+weight_v)/float(2)\n",
    "            Merge_AGraph.add_edge(new_node,neighbor) \n",
    "            Merge_AGraph.edge[new_node][neighbor]['weight'] = new_weight\n",
    "\n",
    "    \n",
    "    # 3) remove old nodes u,v\n",
    "    Merge_AGraph.remove_node(v)\n",
    "    Merge_AGraph.remove_node(u)\n",
    "    return Merge_AGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical\n",
    "# Input: Aggreement Graph, k\n",
    "# Output: m clusters\n",
    "def constrained_hierarchical(AGraph,k): \n",
    "    Copy_AGraph = nx.Graph(AGraph)\n",
    "    removed_nodes = {}\n",
    "    weight_flag = False\n",
    "    while Copy_AGraph.edges and weight_flag == False:\n",
    "        max_edge_weight = -float(\"inf\")\n",
    "        for (u,v) in Copy_AGraph.edges(data=False):\n",
    "            size1 = Copy_AGraph.node[u]['size']\n",
    "            size2 = Copy_AGraph.node[v]['size']\n",
    "            weight = Copy_AGraph.edge[u][v]['weight']\n",
    "            if weight > max_edge_weight and size1+size2 <= k:\n",
    "                new_edge = (u,v)\n",
    "                max_edge_weight = weight\n",
    "        \n",
    "        u = new_edge[0]; v = new_edge[1]\n",
    "        if max_edge_weight != -float(\"inf\"):\n",
    "            Copy_AGraph  = merge_nodes(u,v,Copy_AGraph,'hierarchical')\n",
    "            for (u,data) in Copy_AGraph.nodes(data=True):\n",
    "                size = data['size']\n",
    "                if size == k:\n",
    "                    neighbors = Copy_AGraph.edges(u,data=True)\n",
    "                    removed_nodes[u] = neighbors\n",
    "\n",
    "        else:\n",
    "            weight_flag = True\n",
    "    return Copy_AGraph.nodes(data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hierarchical\n",
    "# Input: Aggreement Graph, k\n",
    "# Output: m clusters\n",
    "def unconstrained_hierarchical(AGraph,k): \n",
    "    Copy_AGraph = nx.Graph(AGraph)\n",
    "    removed_nodes = {}\n",
    "    weight_flag = False\n",
    "    while Copy_AGraph.edges and weight_flag == False:\n",
    "        max_edge_weight = -float(\"inf\")\n",
    "        for (u,v) in Copy_AGraph.edges(data=False):\n",
    "            size1 = Copy_AGraph.node[u]['size']\n",
    "            size2 = Copy_AGraph.node[v]['size']\n",
    "            weight = Copy_AGraph.edge[u][v]['weight']\n",
    "            if weight > max_edge_weight:\n",
    "                new_edge = (u,v)\n",
    "                max_edge_weight = weight\n",
    "        \n",
    "        u = new_edge[0]; v = new_edge[1]\n",
    "        if max_edge_weight != -float(\"inf\") and max_edge_weight >= 0.5:\n",
    "            Copy_AGraph  = merge_nodes(u,v,Copy_AGraph,'hierarchical')\n",
    "            for (u,data) in Copy_AGraph.nodes(data=True):\n",
    "                size = data['size']\n",
    "                if size == k:\n",
    "                    neighbors = Copy_AGraph.edges(u,data=True)\n",
    "                    removed_nodes[u] = neighbors\n",
    "\n",
    "        else:\n",
    "            weight_flag = True\n",
    "    return Copy_AGraph.nodes(data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neighbors \n",
    "# Input: Aggreement Graph, k\n",
    "# Output: m clusters \n",
    "\n",
    "# TODO\n",
    "def neighbors(AGraph,k):\n",
    "    print 'Running neighbors algorithm'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KwikSort\n",
    "# Input: Aggreement Graph, k\n",
    "# Output: m clusters\n",
    "def kwiksort(AGraph,k,person_ids,pairs,num_iter,threshold):\n",
    "    \n",
    "    solution = []\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_output(solution,features,idx_RMSD,idx_a,idx_s,key_names,output_file):\n",
    "    data = []; names_team = []\n",
    "    counter = 1\n",
    "    # print 'Solution in write to output:',solution\n",
    "    max_cluster = -float(\"inf\")\n",
    "    for cluster in solution:\n",
    "        if len(cluster) > max_cluster:\n",
    "            max_cluster = len(cluster)\n",
    "\n",
    "    for idx,cluster in enumerate(solution):\n",
    "        feature_list = []; row = {}; names_team = []\n",
    "        for person in cluster:\n",
    "            name = key_names[person]\n",
    "            names_team.append(name)\n",
    "            features_per_person = features[person]\n",
    "            name\n",
    "            feature_list.append(features_per_person)\n",
    "            \n",
    "        feature_list += [''] * (max_cluster - len(feature_list)+1)\n",
    "        row['cluster'] = counter  \n",
    "        row['cluster size'] = len(cluster)\n",
    "        row['person ids'] = cluster\n",
    "        row['names'] = names_team\n",
    "        row['Avg RMSD'] = idx_RMSD[idx]\n",
    "        row['Avg A'] = idx_a[idx]\n",
    "        row['Avg S'] = idx_s[idx]\n",
    "        for i in range(max_cluster+1):\n",
    "            column_value = 'Member '+str(i)\n",
    "            row[column_value] = feature_list[i]\n",
    "            \n",
    "        data.append(row)   \n",
    "        counter+=1\n",
    "        \n",
    "    output_solution = pd.DataFrame(data=data)\n",
    "    person_ids = output_solution['person ids']; cluster_size = output_solution['cluster size']; \\\n",
    "    cluster = output_solution['cluster']\n",
    "    output_solution.drop(labels=['person ids','cluster size','cluster'], axis=1,inplace = True)\n",
    "    output_solution.insert(0, 'cluster', cluster)\n",
    "    output_solution.insert(1, 'cluster size', cluster_size)\n",
    "    output_solution.insert(2, 'person ids', person_ids)\n",
    "    \n",
    "    output_solution.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findsubsets(features,m):\n",
    "    person_ids = list(features.keys())\n",
    "    return list(itertools.combinations(person_ids, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_small_clusters(min_team_size,max_team_size,solution,features,interests):\n",
    "    new_solution = []\n",
    "    distribute = {}; remove_cluster = []\n",
    "    # creating a distribution of all clusters and \n",
    "    for idx,cluster in enumerate(solution):\n",
    "        if len(cluster) < min_team_size:\n",
    "            for person in cluster:\n",
    "                features_per_person = features[person]\n",
    "                index = features_per_person[8:15].index(1)\n",
    "                interest = interests.keys()[interests.values().index(index)] \n",
    "                distribute[person] = interest\n",
    "            remove_cluster.append(idx)\n",
    "            \n",
    "    for idx in remove_cluster:\n",
    "        del solution[idx]\n",
    "    \n",
    "    for idx,cluster in enumerate(solution):\n",
    "        if len(cluster) < max_team_size and idx not in remove_cluster:\n",
    "            for person in cluster:\n",
    "                features_per_person = features[person]\n",
    "                index = features_per_person[8:15].index(1)\n",
    "                interest = interests.keys()[interests.values().index(index)]\n",
    "                for key,value in distribute.iteritems():\n",
    "                    if interest == 'I have no idea' or interest == value and len(cluster) < max_team_size:\n",
    "                        cluster = cluster + (key,)\n",
    "                        del distribute[key]\n",
    "                        break  \n",
    "        new_solution.append(cluster)\n",
    "            \n",
    "                       \n",
    "    return new_solution\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constrained_KwikSort\n",
    "# Input: Aggreement Graph, k, threshold as percentile\n",
    "# Output: m clusters\n",
    "\n",
    "def constrained_kwiksort(AGraph,k,threshold):\n",
    "    solution = [] \n",
    "    while len(solution)==0:\n",
    "        # print('try kwiksort')\n",
    "        solution = kwiksort_helper(AGraph.copy(),k,threshold)\n",
    "    print('feasible solution found')\n",
    "    return solution\n",
    "\n",
    "# unconstrained_KwikSort\n",
    "# Input: Aggreement Graph, k, threshold\n",
    "# Output: m clusters\n",
    "def unconstrained_kwiksort(AGraph,k,threshold):\n",
    "    # solution should be a list of lists or tuples\n",
    "    solution= []\n",
    "    \n",
    "    ## Fill in here\n",
    "    while (AGraph.number_of_nodes()>k):\n",
    "            \n",
    "        u = AGraph.nodes()[random.randint(0,AGraph.number_of_nodes()-1)]\n",
    "        neighbors = AGraph.neighbors(u)\n",
    "        weights = {}\n",
    "        for neighbor in neighbors:\n",
    "            weights[neighbor] = AGraph.edge[u][neighbor]['weight']\n",
    "        weights = sorted(weights.iteritems(), key=lambda (k,v): (v,k))[::-1]\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            if weights[i][1] < threshold:\n",
    "                cluster = [x[0] for x in weights[:i]]+[u]\n",
    "                solution.append(cluster)\n",
    "                AGraph.remove_nodes_from(cluster)\n",
    "                break\n",
    "            \n",
    "    solution.append(AGraph.nodes())\n",
    "    return solution\n",
    "\n",
    "# helper function\n",
    "def kwiksort_helper(AGraph,k,threshold):\n",
    "    # solution should be a list of lists or tuples\n",
    "    solution= []\n",
    "    fail = 0 \n",
    "    percentile = 70\n",
    "    \n",
    "    ## Fill in here\n",
    "    while (AGraph.number_of_nodes()>k):\n",
    "        # dynamic threshold (70%tile)\n",
    "        dist = []\n",
    "        for edge in AGraph.edges_iter():\n",
    "            dist.append(AGraph.edge[edge[0]][edge[1]]['weight'])\n",
    "        threshold = np.percentile(dist, percentile)\n",
    "        \n",
    "        # sort neighbors according to weights \n",
    "        u = AGraph.nodes()[random.randint(0,AGraph.number_of_nodes()-1)]\n",
    "        neighbors = AGraph.neighbors(u)\n",
    "        weights = {}\n",
    "        for neighbor in neighbors:\n",
    "            weights[neighbor] = AGraph.edge[u][neighbor]['weight']\n",
    "        weights = sorted(weights.iteritems(), key=lambda (k,v): (v,k))[::-1]\n",
    "\n",
    "        # if found enough similar neighbors, form a cluster; o.w. choose another node\n",
    "        if weights[k-2][1] > threshold:\n",
    "            cluster = [x[0] for x in weights[:k-1]] + [u]\n",
    "            solution.append(cluster)\n",
    "            AGraph.remove_nodes_from(cluster)\n",
    "        else: \n",
    "            fail = fail+ 1\n",
    "            if fail > 20: return [] \n",
    "            \n",
    "    solution.append(AGraph.nodes())\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes the centroid from a list of data points\n",
    "def mean(a):\n",
    "    return sum(a) / float(len(a))\n",
    "\n",
    "def centroid(l):\n",
    "    c = map(mean, zip(*l))  \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise(t):\n",
    "    pairs = list(itertools.combinations(t, 2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsd(solution,features):\n",
    "    score = 0\n",
    "    # print 'Solution:',solution\n",
    "    # print 'Features:',features\n",
    "    rmsds = []; clusters_rmsd = []; idx_rmsd = {}\n",
    "    \n",
    "    for idx,team in enumerate(solution):\n",
    "        team_features = []; rmsds = []\n",
    "        for member in team:\n",
    "            team_features.append(features[member])\n",
    "            \n",
    "        team_features = np.array(team_features)\n",
    "        team_features = team_features[:,7:15].tolist()\n",
    "        # team_features = team_features.tolist()\n",
    "        c = centroid(team_features)   \n",
    "        # print 'Centroid is:',c\n",
    "        for person in team_features:\n",
    "            # print 'Person is:',person\n",
    "            # print 'MSE:',mean_squared_error(person, c)\n",
    "            rms = sqrt(mean_squared_error(person, c))\n",
    "            # print 'RMS is:',rms\n",
    "            rmsds.append(rms)\n",
    "            \n",
    "        avg_per_cluster = sum(rmsds)/float(len(rmsds))\n",
    "        idx_rmsd[idx] = avg_per_cluster\n",
    "        # print 'Average cluster is:',avg_per_cluster\n",
    "        clusters_rmsd.append(avg_per_cluster)\n",
    "        # print\n",
    "    \n",
    "    \n",
    "    score = sum(clusters_rmsd)/float(len(clusters_rmsd))\n",
    "    # print 'RMSD Score:',score\n",
    "    return score,clusters_rmsd,idx_rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def silhouette_index(solution,features):\n",
    "    score = 0\n",
    "    a_precomp = {}; a = {}; avg_per_cluster = []; a_avg_of_clusters = []; s_avg_of_clusters = []\n",
    "    b_precomp = {}; b = {}; rms_per_user = []\n",
    "    s = {};\n",
    "    team_members = {}; non_team_members = {};\n",
    "    idx_a = {}; idx_b = {}; idx_s = {}\n",
    "    members = []; users = []\n",
    "    \n",
    "    # list of all users and team members\n",
    "    for idx,team in enumerate(solution):\n",
    "        for member in team:\n",
    "            users.append(member)\n",
    "            copy_team = list(team)\n",
    "            copy_team.remove(member)\n",
    "            remaining_team = copy_team\n",
    "            team_members[member] = remaining_team\n",
    "            \n",
    "    # list of non-team members\n",
    "    for member in users:\n",
    "        team = team_members[member]\n",
    "        non_team = list(set(users) - set(team))\n",
    "        copy_team = list(non_team)\n",
    "        copy_team.remove(member)\n",
    "        non_team_members[member] = copy_team\n",
    "\n",
    "    # computing a - compactness\n",
    "    for idx,team in enumerate(solution):\n",
    "        pairs = pairwise(team)  \n",
    "        sum_sed = 0\n",
    "        for pair in pairs:\n",
    "            u1 = pair[0]; x = features[u1];\n",
    "            u2 = pair[1]; y = features[u2];\n",
    "            rms = sqrt(mean_squared_error(x[7:15], y[7:15]))\n",
    "            # rms = sqrt(mean_squared_error(x, y))\n",
    "            if u1 in a_precomp:\n",
    "                a_precomp[u1].append(rms)\n",
    "            else:\n",
    "                a_precomp[u1] = []\n",
    "                a_precomp[u1].append(rms)\n",
    "                \n",
    "            if u2 in a_precomp:\n",
    "                a_precomp[u2].append(rms)\n",
    "            else:\n",
    "                a_precomp[u2] = []\n",
    "                a_precomp[u2].append(rms)  \n",
    "        \n",
    "        for key,value in a_precomp.iteritems():\n",
    "            avg_per_user = sum(value)/float(len(team) - 1)\n",
    "            # a(i)\n",
    "            a[key] = avg_per_user\n",
    "            avg_per_cluster.append(avg_per_user)\n",
    "            \n",
    "        avg_cluster = sum(avg_per_cluster)/float(len(avg_per_cluster))\n",
    "        idx_a[idx] = avg_cluster\n",
    "        a_avg_of_clusters.append(avg_cluster)\n",
    "        a_precomp = {}; avg_per_cluster = []\n",
    "    \n",
    "    a_score = sum(a_avg_of_clusters)/float(len(a_avg_of_clusters))\n",
    "    \n",
    "    # computing b - separation\n",
    "    for key,value in non_team_members.iteritems():\n",
    "        x = features[key]\n",
    "        for non_member in value:\n",
    "            y = features[non_member]\n",
    "            # rms = sqrt(mean_squared_error(x[7:15], y[7:15]))\n",
    "            rms = sqrt(mean_squared_error(x, y))\n",
    "            rms_per_user.append(rms)\n",
    "\n",
    "        min_dist = min(rms_per_user)\n",
    "        b[key] = min_dist\n",
    "        rms_per_user = []\n",
    "\n",
    "    for key, value in a.iteritems():\n",
    "        # You want a to be small (distance from same cluster points)\n",
    "        # You want b to be large (distance from different cluster points)\n",
    "        if b[key]!=a[key]:\n",
    "            s[key] = (b[key] - a[key])/float(max(b[key],a[key]))\n",
    "        else:\n",
    "            s[key] = 0 \n",
    "    \n",
    "    s_cluster = []; avg_clusters = []\n",
    "    for idx,team in enumerate(solution):\n",
    "        for member in team:\n",
    "            s_cluster.append(s[member])\n",
    "\n",
    "        cluster_avg = sum(s_cluster)/float(len(s_cluster))\n",
    "        idx_s[idx] = cluster_avg\n",
    "        s_avg_of_clusters.append(cluster_avg)\n",
    "        s_cluster = []\n",
    "    \n",
    "    s_score = sum(s_avg_of_clusters)/float(len(s_avg_of_clusters))\n",
    "    return (a_score,s_score,a_avg_of_clusters,s_avg_of_clusters,idx_a,idx_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_features = {}\n",
    "    test_features[1] = [2,1,1,3]; test_features[2] = [1,0,0,1]; test_features[5] = [1,0,0,1]\n",
    "    test_features[3] = [1,0,0,1]; test_features[4] = [1,1,0,0]\n",
    "    team1 = (1,2,5); team2 = (3,4)\n",
    "    test_solution = []; test_solution.append(team1); test_solution.append(team2)\n",
    "    (score_RMSD,clusters_RMSD) = rmsd(test_solution,test_features)\n",
    "    print 'Root Mean Square Deviation for constrained hierarchical is:',score_RMSD\n",
    "    (a_score,s_score,clusters_a,clusters_s) = silhouette_index(test_solution,test_features)\n",
    "    print 'A score for constrained hierarchical is:',a_score\n",
    "    print 'S score for constrained hierarchical is:',s_score\n",
    "    print 'A score cluster averages for constrained hierarchical is:',clusters_a\n",
    "    print 'S score cluster averages for constrained hierarchical is:',clusters_s\n",
    "    print 'Finished with test'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(input_file):\n",
    "    key_names = {}\n",
    "    data = pd.read_csv(input_file)\n",
    "    data = data.dropna(subset=['#'])\n",
    "    data = data.reset_index(drop=True)\n",
    "    # Changing features/skill to binary\n",
    "    data['Beginner'] = data['Beginner'].map({'Beginner': 1, None: 0})\n",
    "    data['Python'] = data['Python'].map({'Python': 1, None: 0})\n",
    "    data['HTML5/CSS'] = data['HTML5/CSS'].map({'HTML5/CSS': 1, None: 0})\n",
    "    data['Javascript'] = data['Javascript'].map({'Javascript': 1, None: 0})\n",
    "    data['Java'] = data['Java'].map({'Java': 1, None: 0})\n",
    "    data['SQL'] = data['SQL'].map({'SQL': 1, None: 0})\n",
    "    data['C++'] = data['C++'].map({'C++': 1, None: 0})\n",
    "    \n",
    "    # Renaming columns\n",
    "    data_names = data.rename(columns=\\\n",
    "                       {'What type of technology are you interested in learning during the hackathon?': 'Technology',\\\n",
    "                        'How many hackathons have you participated in before?': 'NumHackathons',\\\n",
    "                        'What\\'s your first name?':'FirstName',\\\n",
    "                        'Hey {{answer_57551534}}, nice to meet you.What\\'s your last name?':'LastName',\\\n",
    "                        '#': 'Id'})\n",
    "    \n",
    "    # Dropping columns not needed\n",
    "    data = data_names[['Id','NumHackathons','Python','HTML5/CSS','Javascript','Java','SQL','C++','Beginner','Technology']]\n",
    "    technology_list = data['Technology'].tolist()\n",
    "    unique_technologies = list(set(technology_list))\n",
    "    \n",
    "    # Dictionary with key: person id and value: dictionary of features\n",
    "    person_data = {}; skills = []\n",
    "    for index, row in data.iterrows():\n",
    "        skills = []\n",
    "        key = row['Id'];\n",
    "        first_name = data_names.at[index, 'FirstName']\n",
    "        last_name = data_names.at[index, 'LastName']\n",
    "\n",
    "        num_hackathons = row['NumHackathons']; \n",
    "        if row['Python']==1:\n",
    "            skills.append('Python') \n",
    "        if row['HTML5/CSS']==1:\n",
    "            skills.append('HTML5/CSS')\n",
    "        if row['Javascript']==1:\n",
    "            skills.append('Javascript')\n",
    "        if row['Java']==1:\n",
    "            skills.append('Java')\n",
    "        if row['SQL']==1:\n",
    "            skills.append('SQL')\n",
    "        if row['C++']==1:\n",
    "            skills.append('C++')\n",
    "        beginner = 'Beginner' if row['Beginner']==1 else 'Non-Beginner'; technology = row['Technology']\n",
    "        value = {'num_hackathons':num_hackathons,'skills':skills,'experience':beginner,'technology':technology}\n",
    "        person_data[key] = value\n",
    "        key_names[key] = str(first_name)+' '+str(last_name)\n",
    "        \n",
    "    unique_technologies_dict = {}\n",
    "\n",
    "    counter = 0\n",
    "    for technique in unique_technologies:\n",
    "        unique_technologies_dict[technique] = counter\n",
    "        counter+=1\n",
    "    \n",
    "    set_of_technologies = []\n",
    "    for technology in technology_list:\n",
    "        listofzeros = [0] * len(unique_technologies)\n",
    "        index = unique_technologies_dict[technology]\n",
    "        listofzeros[index] = 1\n",
    "        set_of_technologies.append(listofzeros)\n",
    "\n",
    "    # Extracting features 3) Number of hackathos 4) Skills * 6 5) Beginner\n",
    "    #                     6) Technology \n",
    "    # create dictionary of person : [features]\n",
    "    \n",
    "    features = \\\n",
    "            {data.loc[idx, 'Id']: [data.loc[idx, 'NumHackathons'], data.loc[idx, 'Python'], data.loc[idx, 'HTML5/CSS'],\\\n",
    "                                   data.loc[idx, 'Javascript'], data.loc[idx, 'Java'], data.loc[idx, 'SQL'],\\\n",
    "                                   data.loc[idx, 'C++'], data.loc[idx, 'Beginner']] \\\n",
    "                                   for idx in range(data.shape[0])}\n",
    "    \n",
    "    counter = 0\n",
    "    for key,value in features.iteritems():\n",
    "        value.extend(set_of_technologies[counter])\n",
    "        counter+=1\n",
    "        \n",
    "    return data, features, person_data, unique_technologies_dict,key_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(input_file,team_size,min_team_size,num_iter,threshold):\n",
    "    (data, features, person_features,unique,key_names) = data_preprocessing(input_file)\n",
    "    pairs = findsubsets(features,2)\n",
    "    person_ids = list(features.keys())\n",
    "    AGraph = aggreement_graph(features)  \n",
    "    \n",
    "    # IAGraph = nx.Graph(AGraph)\n",
    "    # print 'Running kwiksort'\n",
    "    # solution = kwiksort(IAGraph,team_size,person_ids,pairs,num_iter,threshold)\n",
    "    # write_to_output(solution,person_features,'RecommendedTeamsKwik.csv')\n",
    "    \n",
    "    \n",
    "    # creating test sample for rmsd\n",
    "    # test()\n",
    "\n",
    "    IAGraph = nx.Graph(AGraph)\n",
    "    print 'Running constrained hierarchical'\n",
    "    solution = constrained_hierarchical(IAGraph,team_size)\n",
    "    # print 'Solution:',solution\n",
    "    solution = assign_small_clusters(min_team_size,team_size,solution,features,unique)\n",
    "    print 'Computing Root Mean Square Deviation'\n",
    "    (score_RMSD,clusters_RMSD,idx_RMSD_CH) = rmsd(solution,features)\n",
    "    print 'Root Mean Square Deviation for constrained hierarchical is:',score_RMSD\n",
    "    print 'Computing Silhouette Index'\n",
    "    (a_score,s_score,clusters_a,clusters_s,idx_a_CH,idx_s_CH) = silhouette_index(solution,features)\n",
    "    print 'A score for constrained hierarchical is:',a_score\n",
    "    print 'S score for constrained hierarchical is:',s_score\n",
    "    print 'A score cluster averages for constrained hierarchical is:',clusters_a\n",
    "    print 'S score cluster averages for constrained hierarchical is:',clusters_s\n",
    "    # print 'Writing to file'\n",
    "    write_to_output(solution,person_features,idx_RMSD_CH,idx_a_CH,idx_s_CH,key_names,'ConstrainedHierarch.csv')\n",
    "    \n",
    "    print '*'*50\n",
    "    IAGraph = nx.Graph(AGraph)\n",
    "    print 'Running unconstrained hierarchical'\n",
    "    solution = unconstrained_hierarchical(IAGraph,team_size)\n",
    "    # print 'Solution:',solution\n",
    "    solution = assign_small_clusters(min_team_size,team_size,solution,features,unique)\n",
    "    print 'Computing Root Mean Square Deviation'\n",
    "    (score_RMSD,clusters_RMSD,idx_RMSD_UH) = rmsd(solution,features)\n",
    "    print 'Root Mean Square Deviation for unconstrained hierarchical is:',score_RMSD\n",
    "    print 'Computing Silhouette Index'\n",
    "    (a_score,s_score,clusters_a,clusters_b,idx_a_UH,idx_s_UH) = silhouette_index(solution,features)\n",
    "    print 'A score for unconstrained hierarchical is:',a_score\n",
    "    print 'S score for unconstrained hierarchical is:',s_score\n",
    "    print 'A score cluster averages for unconstrained hierarchical is:',clusters_a\n",
    "    print 'S score cluster averages for unconstrained hierarchical is:',clusters_s\n",
    "    # print 'Writing to file'\n",
    "    write_to_output(solution,person_features,idx_RMSD_UH,idx_a_UH,idx_s_UH,key_names,'UnconstrainedHierarch.csv')\n",
    "    \n",
    "    print '*'*50\n",
    "    con_kwik_rmsd = []\n",
    "    con_kwik_a = []; con_kwik_s = []; con_kwik_clusters_a = []; con_kwik_clusters_s = []\n",
    "    for i in range(6):\n",
    "        IAGraph = nx.Graph(AGraph)\n",
    "        print 'Running constrained kwiksort for time:',i\n",
    "        solution = constrained_kwiksort(IAGraph,team_size,1)\n",
    "        print 'Computing Root Mean Square Deviation'\n",
    "        (score_RMSD,clusters_RMSD,idx_RMSD_CK) = rmsd(solution,features)\n",
    "        con_kwik_rmsd.append(score_RMSD)\n",
    "        print 'Computing Silhouette Index'\n",
    "        (a_score,s_score,clusters_a,clusters_s,idx_a_CK,idx_s_CK) = silhouette_index(solution,features)\n",
    "        con_kwik_a.append(a_score); con_kwik_s.append(s_score) \n",
    "        con_kwik_clusters_a.append(clusters_a); con_kwik_clusters_s.append(clusters_s)\n",
    "    \n",
    "    score_RMSD = sum(con_kwik_rmsd)/float(len(con_kwik_rmsd))\n",
    "    a_score = sum(con_kwik_a)/float(len(con_kwik_a)); s_score = sum(con_kwik_s)/float(len(con_kwik_s))\n",
    "    print 'Root Mean Square Deviation for constrained kwiksort is:',score_RMSD\n",
    "    print 'A score for constrained kwiksort is:',a_score\n",
    "    print 'S score for constrained kwiksort is:',s_score\n",
    "    print 'A score cluster average indicative for constrained kwiksort is:',clusters_a\n",
    "    print 'S score cluster average indicative for constrained kwiksort is:',clusters_s\n",
    "    # print 'Solution:',solution\n",
    "    write_to_output(solution,person_features,idx_RMSD_CK,idx_a_CK,idx_s_CK,key_names,'ConstrainedKwiksort.csv')\n",
    "        \n",
    "#     print '*'*50\n",
    "#     uncon_kwik_rmsd = []\n",
    "#     uncon_kwik_a = []; uncon_kwik_s = []; uncon_kwik_clusters_a = []; uncon_kwik_clusters_s = []\n",
    "#     for i in range(2):\n",
    "#         IAGraph = nx.Graph(AGraph)\n",
    "#         print 'Running unconstrained kwiksort'\n",
    "#         solution = unconstrained_kwiksort(IAGraph,team_size,1/float(2))\n",
    "#         # print 'Solution:',solution\n",
    "#         write_to_output(solution,person_features,'RecommendedTeamsUnconstrainedKwiksort.csv')\n",
    "#         print 'Computing Root Mean Square Deviation'\n",
    "#         (score_RMSD,clusters_RMSD) = rmsd(solution,features)\n",
    "#         uncon_kwik_rmsd.append(score_RMSD)\n",
    "#         print 'Computing Silhouette Index'\n",
    "#         (a_score,s_score,clusters_a,clusters_s) = silhouette_index(solution,features)\n",
    "#         uncon_kwik_a.append(a_score); uncon_kwik_s.append(s_score) \n",
    "#         uncon_kwik_clusters_a.append(clusters_a); uncon_kwik_clusters_s.append(clusters_s)\n",
    "    \n",
    "#     score_RMSD = sum(uncon_kwik_rmsd)/float(len(uncon_kwik_rmsd))\n",
    "#     a_score = sum(uncon_kwik_a)/float(len(uncon_kwik_a)); s_score = sum(uncon_kwik_s)/float(len(uncon_kwik_s))\n",
    "#     print 'Root Mean Square Deviation for unconstrained hierarchical is:',score_RMSD\n",
    "#     print 'A score for unconstrained kwiksort is:',a_score\n",
    "#     print 'S score for unconstrained kwiksort is:',s_score\n",
    "#     print 'A score cluster average indicative for unconstrained kwiksort is:',clusters_a\n",
    "#     print 'S score cluster average indicative for unconstrained kwiksort is:',clusters_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating aggreement graph\n",
      "Running constrained hierarchical\n",
      "Computing Root Mean Square Deviation\n",
      "Root Mean Square Deviation for constrained hierarchical is: 0.0978799372929\n",
      "Computing Silhouette Index\n",
      "A score for constrained hierarchical is: 0.128872529525\n",
      "S score for constrained hierarchical is: 0.232525147845\n",
      "A score cluster averages for constrained hierarchical is: [0.1767766952966369, 0.1767766952966369, 0.0, 0.23570226039551587, 0.0, 0.3173032607475616, 0.23570226039551587, 0.1767766952966369, 0.21213203435596428, 0.23570226039551587, 0.14142135623730953, 0.1767766952966369, 0.0, 0.0, 0.21213203435596428, 0.21213203435596428, 0.1767766952966369, 0.3, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0, 0.0, 0.0]\n",
      "S score cluster averages for constrained hierarchical is: [0.34024758739432637, 0.34024758739432637, 1.0, 0.22081592322841004, 0.25, 0.045375692776911766, -1.0, 0.34024758739432637, -1.0, 0.15397249702656648, 0.4721980699154612, -0.7420614591379635, 1.0, 0.4, 0.02405993581629049, 0.02405993581629049, -1.0, -0.16053743712627053, 1.0, 0.25, 0.3545027756320971, 1.0, 0.5, 1.0, 1.0]\n",
      "**************************************************\n",
      "Running unconstrained hierarchical\n",
      "Computing Root Mean Square Deviation\n",
      "Root Mean Square Deviation for unconstrained hierarchical is: 0.0959474192199\n",
      "Computing Silhouette Index\n",
      "A score for unconstrained hierarchical is: 0.103072881859\n",
      "S score for unconstrained hierarchical is: 0.725894284853\n",
      "A score cluster averages for unconstrained hierarchical is: [0.08249579113843057, 0.0, 0.18519463316790538, 0.20203050891044216, 0.12773541853692472, 0.0, 0.12405382126079786]\n",
      "S score cluster averages for unconstrained hierarchical is: [0.34024758739432637, 0.34024758739432637, 1.0, 0.22081592322841004, 0.25, 0.045375692776911766, -1.0, 0.34024758739432637, -1.0, 0.15397249702656648, 0.4721980699154612, -0.7420614591379635, 1.0, 0.4, 0.02405993581629049, 0.02405993581629049, -1.0, -0.16053743712627053, 1.0, 0.25, 0.3545027756320971, 1.0, 0.5, 1.0, 1.0]\n",
      "**************************************************\n",
      "Running constrained kwiksort for time: 0\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Running constrained kwiksort for time: 1\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Running constrained kwiksort for time: 2\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Running constrained kwiksort for time: 3\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Running constrained kwiksort for time: 4\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Running constrained kwiksort for time: 5\n",
      "feasible solution found\n",
      "Computing Root Mean Square Deviation\n",
      "Computing Silhouette Index\n",
      "Root Mean Square Deviation for constrained kwiksort is: 0.115207012406\n",
      "A score for constrained kwiksort is: 0.155543847189\n",
      "S score for constrained kwiksort is: 0.10883161872\n",
      "A score cluster average indicative for constrained kwiksort is: [0.21213203435596428, 0.21213203435596428, 0.0, 0.0, 0.21213203435596428, 0.14142135623730953, 0.14142135623730953, 0.21213203435596428, 0.21213203435596428, 0.0, 0.0, 0.0, 0.14142135623730953, 0.14142135623730953, 0.0, 0.0, 0.21213203435596428, 0.1414213562373095, 0.21213203435596428, 0.44318516525781365, 0.2, 0.5]\n",
      "S score cluster average indicative for constrained kwiksort is: [0.27882306033351234, -1.0, 0.8, 0.6, 0.024059935816290512, 0.2210079014857449, 0.4721980699154612, -0.24731566091390458, 0.17869967831924338, 0.6, 1.0, 0.6, 0.03471009665666882, -0.1708182628476942, 0.6, 1.0, 0.2589117897614556, 0.4721980699154612, -1.0, -0.893077135854574, 0.04116506890034781, -0.45842535521995426]\n"
     ]
    }
   ],
   "source": [
    "# Team constraints: 1) Each team is a team of approximately 5 2) Similar users and interests are grouped together \n",
    "input_file = 'Hackthegap_clean.csv'\n",
    "dataset_file = 'dataset.csv'\n",
    "num_iter = 1\n",
    "threshold = 1/float(2)\n",
    "# Removing first empty row from dataset -- TODO\n",
    "with open(input_file,'r') as f, open(dataset_file,'w') as f1:\n",
    "    # next(f) # skip header line\n",
    "    for line in f:\n",
    "        f1.write(line)\n",
    "        \n",
    "data = pd.read_csv(dataset_file)      \n",
    "max_team_size = 5\n",
    "min_team_size = 3\n",
    "main(dataset_file, max_team_size, min_team_size, num_iter, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
